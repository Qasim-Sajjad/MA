{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRBAnU9ci9_T"
   },
   "source": [
    "# Mood Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pzp1D8vwi_VJ",
    "outputId": "2fdfb9df-01dd-430c-c334-d58957685148",
    "ExecuteTime": {
     "end_time": "2024-12-16T13:34:45.153893Z",
     "start_time": "2024-12-16T13:34:44.354784Z"
    }
   },
   "source": [
    "!pip install essentia-tensorflow"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: essentia-tensorflow in /opt/anaconda3/envs/music_analysis_env_2/lib/python3.10/site-packages (2.1b6.dev1177)\r\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/anaconda3/envs/music_analysis_env_2/lib/python3.10/site-packages (from essentia-tensorflow) (1.22.4)\r\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/music_analysis_env_2/lib/python3.10/site-packages (from essentia-tensorflow) (1.17.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/music_analysis_env_2/lib/python3.10/site-packages (from essentia-tensorflow) (6.0.2)\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVyTwIPnkw5m",
    "outputId": "9df46238-930f-49aa-86d4-2bb98a35f853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-13 18:10:44--  https://essentia.upf.edu/models/classifiers/danceability/danceability-musicnn-msd-1.pb\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3239625 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘models/mood_detection_models/danceability-musicnn-msd-1.pb’\n",
      "\n",
      "models/mood_detecti 100%[===================>]   3.09M   528KB/s    in 6.6s    \n",
      "\n",
      "2024-12-13 18:10:52 (476 KB/s) - ‘models/mood_detection_models/danceability-musicnn-msd-1.pb’ saved [3239625/3239625]\n",
      "\n",
      "--2024-12-13 18:10:52--  https://essentia.upf.edu/models/classifiers/danceability/danceability-musicnn-msd-1.json\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2677 (2.6K) [application/json]\n",
      "Saving to: ‘metadata/mood_detection_metadatas/danceability-musicnn-msd-1.json’\n",
      "\n",
      "metadata/mood_detec 100%[===================>]   2.61K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-12-13 18:10:53 (27.4 MB/s) - ‘metadata/mood_detection_metadatas/danceability-musicnn-msd-1.json’ saved [2677/2677]\n",
      "\n",
      "--2024-12-13 18:10:53--  https://essentia.upf.edu/models/classifiers/mood_happy/mood_happy-musicnn-msd-1.pb\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3239625 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘models/mood_detection_models/mood_happy-musicnn-msd-1.pb’\n",
      "\n",
      "models/mood_detecti 100%[===================>]   3.09M   491KB/s    in 7.0s    \n",
      "\n",
      "2024-12-13 18:11:01 (453 KB/s) - ‘models/mood_detection_models/mood_happy-musicnn-msd-1.pb’ saved [3239625/3239625]\n",
      "\n",
      "--2024-12-13 18:11:01--  https://essentia.upf.edu/models/classifiers/mood_happy/mood_happy-musicnn-msd-1.json\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3049 (3.0K) [application/json]\n",
      "Saving to: ‘metadata/mood_detection_metadatas/mood_happy-musicnn-msd-1.json’\n",
      "\n",
      "metadata/mood_detec 100%[===================>]   2.98K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-12-13 18:11:03 (189 MB/s) - ‘metadata/mood_detection_metadatas/mood_happy-musicnn-msd-1.json’ saved [3049/3049]\n",
      "\n",
      "--2024-12-13 18:11:03--  https://essentia.upf.edu/models/classifiers/mood_party/mood_party-musicnn-msd-1.pb\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3239625 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘models/mood_detection_models/mood_party-musicnn-msd-1.pb’\n",
      "\n",
      "models/mood_detecti 100%[===================>]   3.09M   519KB/s    in 6.7s    \n",
      "\n",
      "2024-12-13 18:11:11 (470 KB/s) - ‘models/mood_detection_models/mood_party-musicnn-msd-1.pb’ saved [3239625/3239625]\n",
      "\n",
      "--2024-12-13 18:11:11--  https://essentia.upf.edu/models/classifiers/mood_party/mood_party-musicnn-msd-1.json\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3049 (3.0K) [application/json]\n",
      "Saving to: ‘metadata/mood_detection_metadatas/mood_party-musicnn-msd-1.json’\n",
      "\n",
      "metadata/mood_detec 100%[===================>]   2.98K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-12-13 18:11:12 (206 KB/s) - ‘metadata/mood_detection_metadatas/mood_party-musicnn-msd-1.json’ saved [3049/3049]\n",
      "\n",
      "--2024-12-13 18:11:12--  https://essentia.upf.edu/models/classifiers/mood_sad/mood_sad-musicnn-msd-1.pb\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3239625 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘models/mood_detection_models/mood_sad-musicnn-msd-1.pb’\n",
      "\n",
      "models/mood_detecti 100%[===================>]   3.09M   511KB/s    in 6.8s    \n",
      "\n",
      "2024-12-13 18:11:20 (469 KB/s) - ‘models/mood_detection_models/mood_sad-musicnn-msd-1.pb’ saved [3239625/3239625]\n",
      "\n",
      "--2024-12-13 18:11:20--  https://essentia.upf.edu/models/classifiers/mood_sad/mood_sad-musicnn-msd-1.json\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3034 (3.0K) [application/json]\n",
      "Saving to: ‘metadata/mood_detection_metadatas/mood_sad-musicnn-msd-1.json’\n",
      "\n",
      "metadata/mood_detec 100%[===================>]   2.96K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-12-13 18:11:21 (175 MB/s) - ‘metadata/mood_detection_metadatas/mood_sad-musicnn-msd-1.json’ saved [3034/3034]\n",
      "\n",
      "--2024-12-13 18:11:22--  https://essentia.upf.edu/models/classifiers/mood_aggressive/mood_aggressive-musicnn-msd-1.pb\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3239625 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘models/mood_detection_models/mood_aggressive-musicnn-msd-1.pb’\n",
      "\n",
      "models/mood_detecti 100%[===================>]   3.09M   525KB/s    in 6.7s    \n",
      "\n",
      "2024-12-13 18:11:29 (469 KB/s) - ‘models/mood_detection_models/mood_aggressive-musicnn-msd-1.pb’ saved [3239625/3239625]\n",
      "\n",
      "--2024-12-13 18:11:30--  https://essentia.upf.edu/models/classifiers/mood_aggressive/mood_aggressive-musicnn-msd-1.json\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3085 (3.0K) [application/json]\n",
      "Saving to: ‘metadata/mood_detection_metadatas/mood_aggressive-musicnn-msd-1.json’\n",
      "\n",
      "metadata/mood_detec 100%[===================>]   3.01K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-12-13 18:11:31 (45.8 MB/s) - ‘metadata/mood_detection_metadatas/mood_aggressive-musicnn-msd-1.json’ saved [3085/3085]\n",
      "\n",
      "--2024-12-13 18:11:31--  https://essentia.upf.edu/models/classifiers/mood_electronic/mood_electronic-musicnn-msd-1.pb\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3239625 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘models/mood_detection_models/mood_electronic-musicnn-msd-1.pb’\n",
      "\n",
      "models/mood_detecti 100%[===================>]   3.09M   492KB/s    in 7.0s    \n",
      "\n",
      "2024-12-13 18:11:39 (452 KB/s) - ‘models/mood_detection_models/mood_electronic-musicnn-msd-1.pb’ saved [3239625/3239625]\n",
      "\n",
      "--2024-12-13 18:11:39--  https://essentia.upf.edu/models/classifiers/mood_electronic/mood_electronic-musicnn-msd-1.json\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3093 (3.0K) [application/json]\n",
      "Saving to: ‘metadata/mood_detection_metadatas/mood_electronic-musicnn-msd-1.json’\n",
      "\n",
      "metadata/mood_detec 100%[===================>]   3.02K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-12-13 18:11:40 (48.2 MB/s) - ‘metadata/mood_detection_metadatas/mood_electronic-musicnn-msd-1.json’ saved [3093/3093]\n",
      "\n",
      "--2024-12-13 18:11:40--  https://essentia.upf.edu/models/classifiers/mood_relaxed/mood_relaxed-musicnn-msd-1.pb\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3239625 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘models/mood_detection_models/mood_relaxed-musicnn-msd-1.pb’\n",
      "\n",
      "models/mood_detecti 100%[===================>]   3.09M   499KB/s    in 7.0s    \n",
      "\n",
      "2024-12-13 18:11:48 (453 KB/s) - ‘models/mood_detection_models/mood_relaxed-musicnn-msd-1.pb’ saved [3239625/3239625]\n",
      "\n",
      "--2024-12-13 18:11:49--  https://essentia.upf.edu/models/classifiers/mood_relaxed/mood_relaxed-musicnn-msd-1.json\n",
      "Resolving essentia.upf.edu (essentia.upf.edu)... 84.89.139.43\n",
      "Connecting to essentia.upf.edu (essentia.upf.edu)|84.89.139.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3062 (3.0K) [application/json]\n",
      "Saving to: ‘metadata/mood_detection_metadatas/mood_relaxed-musicnn-msd-1.json’\n",
      "\n",
      "metadata/mood_detec 100%[===================>]   2.99K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-12-13 18:11:50 (206 KB/s) - ‘metadata/mood_detection_metadatas/mood_relaxed-musicnn-msd-1.json’ saved [3062/3062]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create directories\n",
    "!mkdir -p models/mood_detection_models\n",
    "!mkdir -p metadata/mood_detection_metadatas\n",
    "\n",
    "# Download .pb file\n",
    "!wget -O models/mood_detection_models/danceability-musicnn-msd-1.pb https://essentia.upf.edu/models/classifiers/danceability/danceability-musicnn-msd-1.pb\n",
    "\n",
    "# Download .json file\n",
    "!wget -O metadata/mood_detection_metadatas/danceability-musicnn-msd-1.json https://essentia.upf.edu/models/classifiers/danceability/danceability-musicnn-msd-1.json\n",
    "\n",
    "# Happy Mood Models\n",
    "!wget -O models/mood_detection_models/mood_happy-musicnn-msd-1.pb https://essentia.upf.edu/models/classifiers/mood_happy/mood_happy-musicnn-msd-1.pb\n",
    "!wget -O metadata/mood_detection_metadatas/mood_happy-musicnn-msd-1.json https://essentia.upf.edu/models/classifiers/mood_happy/mood_happy-musicnn-msd-1.json\n",
    "\n",
    "# Party Mood Models\n",
    "!wget -O models/mood_detection_models/mood_party-musicnn-msd-1.pb https://essentia.upf.edu/models/classifiers/mood_party/mood_party-musicnn-msd-1.pb\n",
    "!wget -O metadata/mood_detection_metadatas/mood_party-musicnn-msd-1.json https://essentia.upf.edu/models/classifiers/mood_party/mood_party-musicnn-msd-1.json\n",
    "\n",
    "# Sad Mood Models\n",
    "!wget -O models/mood_detection_models/mood_sad-musicnn-msd-1.pb https://essentia.upf.edu/models/classifiers/mood_sad/mood_sad-musicnn-msd-1.pb\n",
    "!wget -O metadata/mood_detection_metadatas/mood_sad-musicnn-msd-1.json https://essentia.upf.edu/models/classifiers/mood_sad/mood_sad-musicnn-msd-1.json\n",
    "\n",
    "# Aggressive Mood Models\n",
    "!wget -O models/mood_detection_models/mood_aggressive-musicnn-msd-1.pb https://essentia.upf.edu/models/classifiers/mood_aggressive/mood_aggressive-musicnn-msd-1.pb\n",
    "!wget -O metadata/mood_detection_metadatas/mood_aggressive-musicnn-msd-1.json https://essentia.upf.edu/models/classifiers/mood_aggressive/mood_aggressive-musicnn-msd-1.json\n",
    "\n",
    "# Electronic Mood Models\n",
    "!wget -O models/mood_detection_models/mood_electronic-musicnn-msd-1.pb https://essentia.upf.edu/models/classifiers/mood_electronic/mood_electronic-musicnn-msd-1.pb\n",
    "!wget -O metadata/mood_detection_metadatas/mood_electronic-musicnn-msd-1.json https://essentia.upf.edu/models/classifiers/mood_electronic/mood_electronic-musicnn-msd-1.json\n",
    "\n",
    "# Relaxed Mood Models\n",
    "!wget -O models/mood_detection_models/mood_relaxed-musicnn-msd-1.pb https://essentia.upf.edu/models/classifiers/mood_relaxed/mood_relaxed-musicnn-msd-1.pb\n",
    "!wget -O metadata/mood_detection_metadatas/mood_relaxed-musicnn-msd-1.json https://essentia.upf.edu/models/classifiers/mood_relaxed/mood_relaxed-musicnn-msd-1.json"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-rxPM18rw5Hh",
    "ExecuteTime": {
     "end_time": "2024-12-16T13:34:50.848476Z",
     "start_time": "2024-12-16T13:34:50.811648Z"
    }
   },
   "source": [
    "# import os\n",
    "# import json\n",
    "from essentia.standard import MonoLoader, TensorflowPredictMusiCNN\n",
    "\n",
    "class AudioMoodClassifier:\n",
    "    def __init__(self, models_dir,metadatas_dir):\n",
    "        \"\"\"\n",
    "        Initialize the MoodClassifier with mood models directory and metadatas dir\n",
    "\n",
    "        Args:\n",
    "            models_dir (str): Directory containing model .pb and .json files\n",
    "        \"\"\"\n",
    "        self.models_dir = models_dir\n",
    "        self.metadatas_dir = metadatas_dir\n",
    "        self.models = []\n",
    "        self.models_names = []\n",
    "        self.metadatas = []\n",
    "        self._load_models()\n",
    "\n",
    "    def _load_models(self):\n",
    "        \"\"\"\n",
    "        Load models and their respective json files.\n",
    "        \"\"\"\n",
    "        # Find all .pb and .json files\n",
    "        pb_files = [f for f in os.listdir(self.models_dir) if f.endswith('.pb')]\n",
    "        json_files = [f for f in os.listdir(self.metadatas_dir) if f.endswith('.json')]\n",
    "\n",
    "        # Sort files to ensure matching .pb and .json files\n",
    "        pb_files.sort()\n",
    "        json_files.sort()\n",
    "\n",
    "        print(pb_files)\n",
    "        print(json_files)\n",
    "\n",
    "        # Load metadata and models\n",
    "        for pb_file, json_file in zip(pb_files, json_files):\n",
    "            # Full paths\n",
    "            pb_path = os.path.join(self.models_dir, pb_file)\n",
    "            json_path = os.path.join(self.metadatas_dir, json_file)\n",
    "\n",
    "            # Load metadata\n",
    "            print(f'JSON PATH: {json_path}')\n",
    "            with open(json_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            # Prepare model\n",
    "            model = TensorflowPredictMusiCNN(graphFilename=pb_path)\n",
    "            name = pb_file.replace('.pb','')\n",
    "\n",
    "            # Store model and metadata and names\n",
    "            self.models_names.append(name)\n",
    "            self.models.append(model)\n",
    "            self.metadatas.append(metadata)\n",
    "\n",
    "    def predict(self, audio_path, sample_rate=16000):\n",
    "        \"\"\"\n",
    "        Predict audio Mood Classification.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file\n",
    "            sample_rate (int): Sampling rate for audio loading\n",
    "\n",
    "        Returns:\n",
    "            dict: Classification probabilities for each model\n",
    "        \"\"\"\n",
    "        # Load audio\n",
    "        loader = MonoLoader(sampleRate=sample_rate, filename=audio_path)\n",
    "        audio = loader()\n",
    "\n",
    "        # Predictions from all models.\n",
    "        results = {}\n",
    "\n",
    "        for metadata, model, model_name in zip(self.metadatas, self.models,self.models_names):\n",
    "            # Get model name from the metadata filename\n",
    "            model_name = model_name.replace('-musicnn-msd-1', '')\n",
    "\n",
    "            # Compute model activations and take mean across time\n",
    "            activations = model(audio)\n",
    "            mean_activations = activations.mean(axis=0)\n",
    "\n",
    "            for label, probability in zip(metadata['classes'], mean_activations):\n",
    "              if not label.startswith('non') and not label.startswith('not'):\n",
    "                results[label] = float(f'{100 * probability:.1f}')\n",
    "\n",
    "        #Get Top 3 Predictions.\n",
    "        results = self.get_top_3_predictions(results)\n",
    "\n",
    "        #Return Top 3 Moods Of the Song.\n",
    "        return results\n",
    "\n",
    "    def get_top_3_predictions(self, predictions):\n",
    "        \"\"\"\n",
    "        Get top 3 predictions from a prediction dictionary\n",
    "\n",
    "        Args:\n",
    "            predictions (dict): Predictions dictionary\n",
    "\n",
    "        Returns:\n",
    "            list: Top 3 predictions as [(label, probability)]\n",
    "        \"\"\"\n",
    "        # Sort predictions by probability in descending order\n",
    "        return sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    def print_predictions(self, predictions):\n",
    "        \"\"\"\n",
    "        Print predictions in a formatted manner.\n",
    "\n",
    "        Args:\n",
    "            predictions (dict): Prediction results from predict method\n",
    "        \"\"\"\n",
    "        for model_name, classes in predictions.items():\n",
    "            print(f\"\\n{model_name} Predictions:\")\n",
    "            for label, probability in classes.items():\n",
    "                print(f'{label}: {probability}%')"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TensorflowPredictMusiCNN' from 'essentia.standard' (/opt/anaconda3/envs/music_analysis_env_2/lib/python3.10/site-packages/essentia/standard.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# import os\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# import json\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01messentia\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstandard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MonoLoader, TensorflowPredictMusiCNN\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mAudioMoodClassifier\u001B[39;00m:\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, models_dir,metadatas_dir):\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'TensorflowPredictMusiCNN' from 'essentia.standard' (/opt/anaconda3/envs/music_analysis_env_2/lib/python3.10/site-packages/essentia/standard.py)"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e5xeprR2p7B"
   },
   "source": [
    "# KEY AND BPM DETECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oykcN6ds2snk"
   },
   "outputs": [],
   "source": [
    "import essentia\n",
    "import numpy as np\n",
    "import essentia.standard as estd\n",
    "from essentia.standard import MonoLoader, KeyExtractor\n",
    "import librosa\n",
    "\n",
    "class KeyBPMExtractor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the KeyBPMExtractor with a specific audio file.\n",
    "        \"\"\"\n",
    "        self.audio = None\n",
    "        self.sr = None\n",
    "\n",
    "    def _load_audio(self,file_path):\n",
    "        \"\"\"\n",
    "        Load audio using Essentia's MonoLoader and librosa.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.file_path = file_path\n",
    "            #Feature Extraction for tonal based Approaach of extraction of key/BPM from audio.\n",
    "            self.features, self.features_frames = estd.MusicExtractor(lowlevelStats=['mean', 'stdev'],\n",
    "                                                          rhythmStats=['mean', 'stdev'],\n",
    "                                                          tonalStats=['mean', 'stdev'])(self.file_path)\n",
    "\n",
    "            # Load with librosa to get sample rate for BPM Detection.\n",
    "            self.y, self.sr = librosa.load(self.file_path, sr=None)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {self.file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_key(self):\n",
    "        \"\"\"\n",
    "        Extract the musical key of the audio file.\n",
    "\n",
    "        Returns:\n",
    "            str: Formatted key and scale (e.g., \"C Major\")\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            # key_extractor = KeyExtractor()\n",
    "            # key, scale, _ = key_extractor(self.audio)\n",
    "\n",
    "            keys,scales = [],[]\n",
    "\n",
    "            #Get Key from tonal.key_edma.key for analysis.\n",
    "            keys.append(self.features['tonal.key_edma.key'])\n",
    "            scales.append(self.features['tonal.key_edma.scale'])\n",
    "\n",
    "            #Get Key from tonal.key_krumhansl.key for analysis.\n",
    "            keys.append(self.features['tonal.key_krumhansl.key'])\n",
    "            scales.append(self.features['tonal.key_krumhansl.scale'])\n",
    "\n",
    "            #Get Key from tonal.key_temperley.key for analysis.\n",
    "            keys.append(self.features['tonal.key_temperley.key'])\n",
    "            scales.append(self.features['tonal.key_temperley.scale'])\n",
    "\n",
    "            #Get Key from librosa.\n",
    "            chroma = librosa.feature.chroma_cqt(y=self.y, sr=self.sr)\n",
    "            chroma_mean = np.mean(chroma,axis=1)\n",
    "            lib_keys = [\n",
    "                \"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"\n",
    "            ]\n",
    "            key_index = np.argmax(chroma_mean)\n",
    "            predicted_key = lib_keys[key_index]\n",
    "\n",
    "            detection_key_scales = [f\"{key} {scale}\" for key, scale in zip(keys, scales)]\n",
    "\n",
    "            detection_key_scales.append(predicted_key)\n",
    "            print(f'Detected Key Scales are: {detection_key_scales}')\n",
    "\n",
    "            return detection_key_scales\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting key for {self.file_path}: {e}\")\n",
    "            return \"Key Not Found\"\n",
    "\n",
    "    def extract_bpm(self):\n",
    "        \"\"\"\n",
    "        Extract the tempo (BPM) of the audio file.\n",
    "\n",
    "        Returns:\n",
    "            int: Rounded beats per minute\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bpms = []\n",
    "            #Extracting BPM using Essentia.\n",
    "            bpm = self.features['rhythm.bpm']\n",
    "            bpms.append(round(float(bpm)))\n",
    "\n",
    "            #Extracting BPM using librosa.beat_track.\n",
    "            tempo, _ = librosa.beat.beat_track(y=self.y, sr=self.sr)\n",
    "            bpms.append(round(float(tempo)))\n",
    "\n",
    "            #Using librosa.beat.tempo for analysis now.\n",
    "            tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n",
    "\n",
    "            bpms.append(round(float(tempo)))\n",
    "\n",
    "            return bpms\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting BPM for {self.file_path}: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def analyze(self,file_path):\n",
    "        \"\"\"\n",
    "        Perform complete audio analysis.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing analysis results\n",
    "        \"\"\"\n",
    "        self._load_audio(file_path)\n",
    "\n",
    "        results = {\n",
    "            'Key': self.extract_key(),\n",
    "            'BPM' : self.extract_bpm()\n",
    "        }\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFgTSsex8jo_"
   },
   "source": [
    "# Instrument Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IrypXC6C9z45"
   },
   "outputs": [],
   "source": [
    "# Download the model file into the models directory\n",
    "!wget -q https://essentia.upf.edu/models/classification-heads/mtg_jamendo_instrument/mtg_jamendo_instrument-discogs-effnet-1.pb -O models/mtg_jamendo_instrument-discogs-effnet-1.pb\n",
    "\n",
    "# Download the metadata file into the metadata directory\n",
    "!wget -q https://essentia.upf.edu/models/classification-heads/mtg_jamendo_instrument/mtg_jamendo_instrument-discogs-effnet-1.json -O metadata/mtg_jamendo_instrument-discogs-effnet-1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "XM4c7GG4-3PJ"
   },
   "outputs": [],
   "source": [
    "# Download the model file into the models directory\n",
    "!wget -q https://essentia.upf.edu/models/feature-extractors/discogs-effnet/discogs-effnet-bs64-1.pb -O models/discogs-effnet-bs64-1.pb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q_9FjiC8DFc9",
    "ExecuteTime": {
     "end_time": "2024-12-17T20:41:45.536404Z",
     "start_time": "2024-12-17T20:41:26.760758Z"
    }
   },
   "source": [
    "import json\n",
    "from essentia.standard import MonoLoader, TensorflowPredictEffnetDiscogs, TensorflowPredict2D\n",
    "\n",
    "class AudioInstrumentClassifier:\n",
    "    def __init__(self,instrument_model_path,model_json_path,embedding_model_path):\n",
    "\n",
    "        self.models = []\n",
    "        self.models_names = []\n",
    "        self.metadatas = []\n",
    "        self.instrument_model_path = instrument_model_path\n",
    "        self.model_json_path = model_json_path\n",
    "        self.embedding_model_path = embedding_model_path\n",
    "\n",
    "    #Load Instrument Model and its JSON Metadata.\n",
    "    def load_instrument_model(self):\n",
    "\n",
    "        # Load Instrument Prediction Model and its Metadata Path\n",
    "        model_path = self.instrument_model_path\n",
    "        metadata_path = self.model_json_path\n",
    "\n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Prepare model\n",
    "        model = TensorflowPredict2D(graphFilename=model_path)\n",
    "\n",
    "        return model,metadata\n",
    "\n",
    "    #Load Embeddings of the audio.\n",
    "    def load_embeddings(self,audio):\n",
    "\n",
    "      embedding_model_path = self.embedding_model_path\n",
    "      embedding_model = TensorflowPredictEffnetDiscogs(graphFilename=embedding_model_path, output=\"PartitionedCall:1\")\n",
    "      embeddings = embedding_model(audio)\n",
    "\n",
    "      return embeddings\n",
    "\n",
    "    def predict(self, audio_path, sample_rate=16000):\n",
    "\n",
    "        # Load audio\n",
    "        loader = MonoLoader(sampleRate=sample_rate, filename=audio_path)\n",
    "        audio = loader()\n",
    "\n",
    "        #Load embeddings from Discog model.\n",
    "        embeddings = self.load_embeddings(audio)\n",
    "\n",
    "        #Load Instrument Model and MetaData.\n",
    "        model,metadata = self.load_instrument_model()\n",
    "\n",
    "        predictions = model(embeddings)\n",
    "\n",
    "        #Take Mean across-each timeStamp\n",
    "        predictions = predictions.mean(axis=0)\n",
    "        results = {}\n",
    "\n",
    "        for label, probability in zip(metadata['classes'], predictions):\n",
    "            results[label] = float(f'{100 * probability:.1f}')\n",
    "\n",
    "        #Get Top3 Instruments within Music.\n",
    "        results = self.get_top_3_predictions(results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_top_3_predictions(self, predictions):\n",
    "        \"\"\"\n",
    "        Get top 3 predictions from a prediction dictionary\n",
    "\n",
    "        Args:\n",
    "            predictions (dict): Predictions dictionary\n",
    "\n",
    "        Returns:\n",
    "            list: Top 3 predictions as [(label, probability)]\n",
    "        \"\"\"\n",
    "        # Sort predictions by probability in descending order\n",
    "        return sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    def print_predictions(self, predictions):\n",
    "\n",
    "      print(f\"\\nInstrument Model Predictions:\")\n",
    "      for label, probability in predictions.items():\n",
    "          print(f'{label}: {probability}%')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to import NumPy C API from Essentia module. Error code = -1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01messentia\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstandard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MonoLoader, TensorflowPredictEffnetDiscogs, TensorflowPredict2D\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mAudioInstrumentClassifier\u001B[39;00m:\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m,instrument_model_path,model_json_path,embedding_model_path):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/music_analysis_env_2/lib/python3.10/site-packages/essentia/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _essentia\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_essentia\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m reset\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'numpy.core'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6fyG9u5GZ4u"
   },
   "source": [
    "# GENRE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fLU60F85wL_L"
   },
   "outputs": [],
   "source": [
    "# Download the mtg-jamendo-genre-model file into the models directory\n",
    "!wget -q https://essentia.upf.edu/models/classification-heads/mtg_jamendo_genre/mtg_jamendo_genre-discogs-effnet-1.pb -O models/mtg_jamendo_genre-discogs-effnet-1.pb\n",
    "# Download the mtg-jamendo-metadata file into the metadata directory\n",
    "!wget -q https://essentia.upf.edu/models/classification-heads/mtg_jamendo_genre/mtg_jamendo_genre-discogs-effnet-1.json -O metadata/mtg_jamendo_genre-discogs-effnet-1.json\n",
    "\n",
    "#Download the msd-musicnn model and metadata.\n",
    "!wget -q https://essentia.upf.edu/models/autotagging/msd/msd-musicnn-1.pb -O models/msd-musicnn-1.pb\n",
    "!wget -q https://essentia.upf.edu/models/autotagging/msd/msd-musicnn-1.json -O metadata/msd-musicnn-1.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6q18FilUvFuT"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from essentia.standard import MonoLoader, TensorflowPredictEffnetDiscogs, TensorflowPredict2D,TensorflowPredictMusiCNN , TensorflowPredictVGGish\n",
    "\n",
    "class AudioGenreClassifier:\n",
    "    def __init__(self,\n",
    "                 genre_model_path,\n",
    "                 model_json_path,\n",
    "                 essentia_genre_model_path,\n",
    "                 essentia_genre_json_path,\n",
    "                 embedding_model_path):\n",
    "\n",
    "        self.models = []\n",
    "        self.models_names = []\n",
    "        self.metadatas = []\n",
    "        #Genre Models From MTG-Jamendro Dataset.\n",
    "        self.genre_model_path = genre_model_path\n",
    "        self.model_json_path = model_json_path\n",
    "\n",
    "        #Genre Model Ussing Essentia AutoTagging.\n",
    "        self.essentia_genre_model_path = essentia_genre_model_path\n",
    "        self.essentia_genre_json_path = essentia_genre_json_path\n",
    "\n",
    "        #Embedding Model for MTG.\n",
    "        self.embedding_model_path = embedding_model_path\n",
    "\n",
    "    def load_genre_model(self):\n",
    "\n",
    "        # Load Genre Prediction Model and its Metadata Path\n",
    "        model_path = self.genre_model_path\n",
    "        metadata_path = self.model_json_path\n",
    "\n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Prepare model\n",
    "        model = TensorflowPredict2D(graphFilename=model_path)\n",
    "\n",
    "        return model,metadata\n",
    "\n",
    "    def load_essentia_autotagging_model(self):\n",
    "\n",
    "      with open(self.essentia_genre_json_path, 'r') as f:\n",
    "          metadata = json.load(f)\n",
    "\n",
    "      model = TensorflowPredictMusiCNN(graphFilename=self.essentia_genre_model_path)\n",
    "\n",
    "      return model,metadata\n",
    "\n",
    "    def load_embeddings(self,audio):\n",
    "\n",
    "      embedding_model_path = self.embedding_model_path\n",
    "      embedding_model = TensorflowPredictEffnetDiscogs(graphFilename=embedding_model_path, output=\"PartitionedCall:1\")\n",
    "      embeddings = embedding_model(audio)\n",
    "\n",
    "      return embeddings\n",
    "\n",
    "    def get_top_3_predictions(self, predictions):\n",
    "        \"\"\"\n",
    "        Get top 3 predictions from a prediction dictionary\n",
    "\n",
    "        Args:\n",
    "            predictions (dict): Predictions dictionary\n",
    "\n",
    "        Returns:\n",
    "            list: Top 3 predictions as [(label, probability)]\n",
    "        \"\"\"\n",
    "        # Sort predictions by probability in descending order\n",
    "        return sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    def predict(self, audio_path, sample_rate=16000):\n",
    "\n",
    "        # Load audio\n",
    "        loader = MonoLoader(sampleRate=sample_rate, filename=audio_path)\n",
    "        audio = loader()\n",
    "\n",
    "        #Load embeddings from Discog model.\n",
    "        embeddings = self.load_embeddings(audio)\n",
    "\n",
    "        #Load Genre Model and MetaData.\n",
    "        model,metadata = self.load_genre_model()\n",
    "\n",
    "        #Load Essentia Auto-Tagging Model and MetaData.\n",
    "        essentia_model, essentia_metadata = self.load_essentia_autotagging_model()\n",
    "\n",
    "\n",
    "        #Take Mean across-each timeStamp for mtg.\n",
    "        mtg_predictions = model(embeddings)\n",
    "        mtg_predictions = mtg_predictions.mean(axis=0)\n",
    "        mtg_results = {}\n",
    "        for label, probability in zip(metadata['classes'], mtg_predictions):\n",
    "          mtg_results[label] = float(f'{100 * probability:.1f}')\n",
    "\n",
    "        #Take Mean across-each timeStamp for essentia.\n",
    "        essentia_predictions = essentia_model(audio)\n",
    "        essentia_predictions = essentia_predictions.mean(axis=0)\n",
    "        essentia_genre_results = {}\n",
    "        for label, probability in zip(essentia_metadata['classes'], essentia_predictions):\n",
    "          essentia_genre_results[label] = float(f'{100 * probability:.1f}')\n",
    "\n",
    "        #Get Top3 Genres Of Music.\n",
    "        mtg_results = self.get_top_3_predictions(mtg_results)\n",
    "        essentia_genre_results = self.get_top_3_predictions(essentia_genre_results)\n",
    "\n",
    "        return mtg_results , essentia_genre_results\n",
    "\n",
    "    def print_predictions(self, predictions):\n",
    "\n",
    "      print(f\"\\Genre Model Predictions:\")\n",
    "      for label, probability in predictions.items():\n",
    "          print(f'{label}: {probability}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEEGTsbK4ptN"
   },
   "source": [
    "# SENTIMENT ANALYSIS AND LYRICS EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPM45zXYA1on",
    "outputId": "2504f052-ea89-4eb8-d9b0-79bd55435bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nXp5E9WU4tXS"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login, InferenceClient\n",
    "# from dotenv import load_dotenv\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "#LYRICS BASED SENTIMENT ANALYSIS.\n",
    "\n",
    "class LyricsExtractor:\n",
    "    def __init__(self, model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"):\n",
    "        \"\"\"\n",
    "        Initializes the sentiment analyzer with a specified model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Hugging Face model name for LLM. Defaults to \"mistralai/Mixtral-8x7B-Instruct-v0.1\".\n",
    "        \"\"\"\n",
    "\n",
    "        #Login to HuggingFace Hub.\n",
    "        login(token=userdata.get('hf_mixtral_tk'),add_to_git_credential=True)\n",
    "\n",
    "        # Using Mixtral , because it ranks great at chatbot arena, for generation of QA Pairs.\n",
    "        repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "        self.llm_client = InferenceClient(\n",
    "            model=repo_id,\n",
    "            timeout=200,\n",
    "        )\n",
    "\n",
    "\n",
    "    def call_llm(self,\n",
    "                 inference_client: InferenceClient,\n",
    "                 prompt: str,\n",
    "                 max_tokens:int):\n",
    "\n",
    "        response = inference_client.post(\n",
    "            json={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\"max_new_tokens\": max_tokens},\n",
    "                \"task\": \"text-generation\",\n",
    "            },\n",
    "        )\n",
    "        return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "    def analyze_sentiment(self, lyrics_text, max_new_tokens=60):\n",
    "        \"\"\"\n",
    "        Analyzes the sentiment of the provided lyrics.\n",
    "\n",
    "        Args:\n",
    "            lyrics_text (str): Lyrics text to analyze.\n",
    "            max_new_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            str: Sentiment analysis generated by the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create the prompt for sentiment analysis\n",
    "        prompt = (\n",
    "            f\"Below are some song lyrics:\\n\\n{lyrics_text}\\n\\n\"\n",
    "            \"Analyze the text above and describe the overall sentiment and mood of these lyrics in one short and concise sentence. Provide your response after the [RESULT] token.\\n\\n [RESULT]\"\n",
    "        )\n",
    "\n",
    "        # Generate the result\n",
    "        result_text = self.call_llm(self.llm_client, prompt, max_new_tokens)\n",
    "\n",
    "        # Return the generated text\n",
    "        return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "id": "o_I7nJL8HpD4",
    "outputId": "0a89da2a-9fab-4d60-aba6-62ddc1dc8082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-aiplatform 1.73.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-bigquery-connection 1.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-functions 1.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-iam 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-language 2.15.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-pubsub 2.27.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-resource-manager 1.13.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-translate 3.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "googleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n",
      "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
      "tensorflow-datasets 4.9.7 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.9.3 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mCollecting protobuf==3.20.3\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
      "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow 2.9.3 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.9.3 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed protobuf-3.20.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "f05b734c043c40b4aad394047fee22cb",
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.10/dist-packages (2.0.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q spleeter\n",
    "!pip install protobuf==3.20.3\n",
    "!pip install -q openai-whisper\n",
    "!pip install webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-0p0qpmMHDPD"
   },
   "outputs": [],
   "source": [
    "# SENTIMENT ANALYSIS\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import webrtcvad\n",
    "from spleeter.separator import Separator\n",
    "import whisper\n",
    "from transformers import pipeline\n",
    "\n",
    "class AudioSentimentAnalyzer:\n",
    "    def __init__(self,\n",
    "                 LyricsExtractor,\n",
    "                 model_dir=None):\n",
    "        \"\"\"\n",
    "        Initialize the AudioSentimentAnalyzer.\n",
    "\n",
    "        Args:\n",
    "            model_dir (str, optional): Directory to store temporary files\n",
    "        \"\"\"\n",
    "        self.model_dir = model_dir or os.path.join(os.getcwd(), 'audio_analysis_temp')\n",
    "        self.Lyrics_Analyzer = LyricsExtractor\n",
    "\n",
    "        # Ensure temporary directory exists\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize models\n",
    "        self._init_models()\n",
    "\n",
    "    def _init_models(self):\n",
    "        \"\"\"\n",
    "        Initialize required models.\n",
    "        \"\"\"\n",
    "        # Vocal Separator\n",
    "        self.separator = Separator('spleeter:2stems')\n",
    "\n",
    "        # Whisper for transcription\n",
    "        self.whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "        # Sentiment Analysis Pipeline\n",
    "        self.sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    def check_vocals_presence(self,audio_to_check_vocals,aggressiveness):\n",
    "        \"\"\"\n",
    "        Check if vocals are present in the audio file.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if vocals are present, False otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'Vocals Audio File Path:{audio_to_check_vocals}')\n",
    "        #Load the Audio File in Librosa and check frequencies that whether it has lyrics or not.\n",
    "        self.y ,self.sr = librosa.load(audio_to_check_vocals,sr=16000,mono=True)\n",
    "\n",
    "        # Convert to 16-bit PCM\n",
    "        audio = (self.y * 32767).astype(np.int16)\n",
    "\n",
    "        # Initialize VAD\n",
    "        vad = webrtcvad.Vad(aggressiveness)\n",
    "\n",
    "        # Frame the audio (30ms frames - recommended for VAD)\n",
    "        frame_duration = 30  # ms\n",
    "        frame_size = int(self.sr * (frame_duration / 1000))\n",
    "\n",
    "        # Count voice frames\n",
    "        voice_frames = 0\n",
    "        total_frames = 0\n",
    "\n",
    "        for start in range(0, len(audio), frame_size):\n",
    "            frame = audio[start:start+frame_size]\n",
    "\n",
    "            # Ensure frame is the correct length\n",
    "            if len(frame) == frame_size:\n",
    "                total_frames += 1\n",
    "                try:\n",
    "                    if vad.is_speech(frame.tobytes(), self.sr):\n",
    "                        voice_frames += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing frame: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Calculate vocal presence ratio\n",
    "        vocal_ratio = voice_frames / total_frames if total_frames > 0 else 0\n",
    "\n",
    "        return {\n",
    "            'has_vocals': vocal_ratio > 0.2,\n",
    "            'vocal_ratio': vocal_ratio,\n",
    "            'total_frames': total_frames,\n",
    "            'voice_frames': voice_frames\n",
    "        }\n",
    "\n",
    "\n",
    "    def separate_vocals(self,audio_path):\n",
    "        \"\"\"\n",
    "        Separate vocals from the audio file.\n",
    "\n",
    "        Returns:\n",
    "            str: Path to extracted vocals file or None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Separate to temporary directory\n",
    "            output_dir = os.path.join(self.model_dir, 'separated')\n",
    "            self.separator.separate_to_file(audio_path, output_dir)\n",
    "\n",
    "            # Find vocals file\n",
    "            vocals_path = os.path.join(output_dir, os.path.splitext(os.path.basename(audio_path))[0], 'vocals.wav')\n",
    "\n",
    "            return vocals_path if os.path.exists(vocals_path) else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error separating vocals: {e}\")\n",
    "            return None\n",
    "\n",
    "    def transcribe_vocals(self, vocals_path):\n",
    "        \"\"\"\n",
    "        Transcribe vocals to text.\n",
    "\n",
    "        Args:\n",
    "            vocals_path (str): Path to vocals audio file\n",
    "\n",
    "        Returns:\n",
    "            str: Transcribed lyrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.whisper_model.transcribe(vocals_path)\n",
    "            return result[\"text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error transcribing vocals: {e}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to analyze\n",
    "\n",
    "        Returns:\n",
    "            dict: Sentiment analysis results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # If text is too short or empty, return None\n",
    "            if not text or len(text.strip()) < 5:\n",
    "                return None\n",
    "\n",
    "            # Use sentiment analysis pipeline\n",
    "            sentiment_result = self.sentiment_analyzer(text)[0]\n",
    "            return sentiment_result\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing sentiment: {e}\")\n",
    "            return None\n",
    "\n",
    "    def analyze(self,audio_path, moods):\n",
    "        \"\"\"\n",
    "        Perform comprehensive audio analysis.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to audio file\n",
    "            moods (str, optional): Predefined mood if no lyrics found\n",
    "\n",
    "        Returns:\n",
    "            dict: Comprehensive analysis results\n",
    "        \"\"\"\n",
    "\n",
    "        # Separate vocals from music first, then check whther the serperated file actually has vocals in it.\n",
    "        vocals_path = self.separate_vocals(audio_path)\n",
    "\n",
    "        #Check for Vocals Presence.\n",
    "        vocals_features = self.check_vocals_presence(vocals_path,aggressiveness=3)\n",
    "\n",
    "        # Initialize result dictionary\n",
    "        result = {}\n",
    "        has_lyrics = False\n",
    "\n",
    "        if vocals_features['has_vocals']:\n",
    "\n",
    "          # Attempt lyric transcription\n",
    "          lyrics = None\n",
    "          if vocals_path:\n",
    "              lyrics = self.transcribe_vocals(vocals_path)\n",
    "\n",
    "              sentiment = self.Lyrics_Analyzer.analyze_sentiment(lyrics)\n",
    "              result.update({\n",
    "                  'lyrics': lyrics,\n",
    "                  'sentiment': sentiment\n",
    "              })\n",
    "\n",
    "              has_lyrics = True\n",
    "        else:\n",
    "            # If no lyrics, use predefined mood\n",
    "            mood_prompt = f\"Describe the sentiment of a song with following {moods[0]},{moods[1]}, {moods[2]} moods.\"\n",
    "            sentiment = self.analyze_sentiment(mood_prompt)\n",
    "            result.update({\n",
    "                'lyrics': \"No Lyrics, Sentiment Based on Mood\",\n",
    "                'sentiment': f\"The Sentiment of the Music is {sentiment['label']}\"\n",
    "            })\n",
    "\n",
    "        return result, has_lyrics\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Clean up temporary files and directories.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import shutil\n",
    "            if os.path.exists(self.model_dir):\n",
    "                shutil.rmtree(self.model_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "# Lyrics = LyricsExtractor()\n",
    "# sentiment_anz = AudioSentimentAnalyzer(LyricsExtractor=Lyrics)\n",
    "# sentiment_anz.analyze(audio_path='audio_files/A Beacon of Hope.mp3',moods=['happy','sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcX2Hs__Tv9I",
    "outputId": "23a69be0-03ff-406d-ae60-2e67f93e2e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (3.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iisNW0krHVfa"
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1bPaF5TfHXBy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def process_directory(audio_dir,\n",
    "                      output_path,\n",
    "                      genre_classifier,\n",
    "                      mood_classifier,\n",
    "                      Instrument_classifier,\n",
    "                      KeyBPM,\n",
    "                      Sentiment_analyzer):\n",
    "    \"\"\"\n",
    "    Process audio files, skipping those already processed\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Try reading with different encodings\n",
    "    encodings_to_try = ['latin-1', 'iso-8859-1', 'cp1252']\n",
    "    processed_files = set()\n",
    "\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            existing_df = pd.read_csv(output_path, encoding=encoding)\n",
    "            print(f\"Successfully read file with {encoding} encoding\")\n",
    "            processed_files = set(existing_df['Filename'])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read with {encoding} encoding: {e}\")\n",
    "\n",
    "    # If CSV doesn't exist, create it with headers\n",
    "    if not os.path.exists(output_path):\n",
    "        with open(output_path, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            header = ['Filename',\n",
    "                      'Essentia-Autotagging-Genre1', 'Genre1_Prob',\n",
    "                      'Essentia-Autotagging-Genre2', 'Genre2_Prob',\n",
    "                      'Essentia-Autotagging-Genre3', 'Genre3_Prob',\n",
    "                      'MTG-Jamendo-Genre1', 'Genre1_Prob',\n",
    "                      'MTG-Jamendo-Genre2', 'Genre2_Prob',\n",
    "                      'MTG-Jamendo-Genre3', 'Genre3_Prob',\n",
    "                      'Essentia-Mood1', 'Mood1_Prob',\n",
    "                      'Essentia-Mood2', 'Mood2_Prob',\n",
    "                      'Essentia-Mood3', 'Mood3_Prob',\n",
    "                      'MTG-Jamendo-Instrument1', 'Instrument1_Prob',\n",
    "                      'MTG-Jamendo-Instrument2', 'Instrument2_Prob',\n",
    "                      'MTG-Jamend0-Instrument3', 'Instrument3_Prob',\n",
    "                      'essentia.Key_edma',\n",
    "                      'essentia.key_krumhansl',\n",
    "                      'essentia.key_temperley',\n",
    "                      'librosa.key',\n",
    "                      'essentia.rhythm.bpm',\n",
    "                      'librosa.beat_track (BPM)',\n",
    "                      'librosa.beat.tempo (BPM)',\n",
    "                      'Lyrics',\n",
    "                      'Sentiment Anaylsis']\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "    # Prepare results dictionary\n",
    "    all_results = {}\n",
    "\n",
    "    # Process each audio file in the directory\n",
    "    for filename in os.listdir(audio_dir):\n",
    "        # Skip if file already processed\n",
    "        if filename in processed_files:\n",
    "            print(f\"Skipping already processed file: {filename}\")\n",
    "            continue\n",
    "\n",
    "        if filename.endswith(('.wav', '.mp3', '.flac')):  # Add more extensions if needed\n",
    "            file_path = os.path.join(audio_dir, filename)\n",
    "\n",
    "            try:\n",
    "                # Get Top3 Genre, Mood and Instruments for the Music File\n",
    "                mtg_genre_predictions , essentia_tagging_predictions  = genre_classifier.predict(file_path)\n",
    "                print(f'MTG-Genre Predictions for {filename}: {mtg_genre_predictions}')\n",
    "                print(f'Essentia-Genre Predictions for {filename}: {essentia_tagging_predictions}')\n",
    "\n",
    "\n",
    "                mood_predictions = mood_classifier.predict(file_path)\n",
    "                print(f'Mood Predictions for {filename}: {mood_predictions}')\n",
    "\n",
    "                instrument_predictions = Instrument_classifier.predict(file_path)\n",
    "                print(f'Instrument Predictions for {filename}: {instrument_predictions}')\n",
    "\n",
    "                # Get Key and BPM\n",
    "                key_n_bpm = KeyBPM.analyze(file_path=file_path)\n",
    "\n",
    "                # Sentiment Analysis\n",
    "                sentiment_results, has_lyrics = Sentiment_analyzer.analyze(\n",
    "                    file_path,\n",
    "                    moods=[mood for mood, prob in mood_predictions[:]]\n",
    "                )\n",
    "\n",
    "                # Process sentiment if lyrics exist\n",
    "                if has_lyrics:\n",
    "                    result_token = \"[RESULT]\"\n",
    "                    sentiment_results['sentiment'] = sentiment_results['sentiment'].split(result_token, 1)[1].strip()\n",
    "                    sentiment_results['sentiment'] = sentiment_results['sentiment'].split(result_token)[1]\n",
    "\n",
    "                print(f\"Sentiment for {filename}: {sentiment_results['sentiment']}\")\n",
    "\n",
    "                # Prepare row for CSV\n",
    "                row = [filename]\n",
    "\n",
    "                # Add MTG-genres\n",
    "                for genre, prob in mtg_genre_predictions:\n",
    "                    row.extend([genre, prob])\n",
    "                # Pad with empty values if less than 3 genres\n",
    "                while len(row) < 7:\n",
    "                    row.extend(['', ''])\n",
    "\n",
    "                # Add Essentia-AutoTagging genres\n",
    "                for genre, prob in essentia_tagging_predictions:\n",
    "                    row.extend([genre, prob])\n",
    "                # Pad with empty values if less than 3 genres\n",
    "                while len(row) < 13:\n",
    "                    row.extend(['', ''])\n",
    "\n",
    "                # Add moods\n",
    "                for mood, prob in mood_predictions:\n",
    "                    row.extend([mood, prob])\n",
    "                # Pad with empty values if less than 3 moods\n",
    "                while len(row) < 19:\n",
    "                    row.extend(['', ''])\n",
    "\n",
    "                # Add instruments\n",
    "                for instrument, prob in instrument_predictions:\n",
    "                    row.extend([instrument, prob])\n",
    "                # Pad with empty values if less than 3 instruments\n",
    "                while len(row) < 25:\n",
    "                    row.extend(['', ''])\n",
    "\n",
    "                # Add keys\n",
    "                for key in key_n_bpm['Key']:\n",
    "                    row.extend([key])\n",
    "                # Pad with empty values if less than 4 keys\n",
    "                while len(row) < 29:\n",
    "                    row.append('')\n",
    "\n",
    "                for bpm in key_n_bpm['BPM']:\n",
    "                    row.extend([bpm])\n",
    "                # Pad with empty values if less than 3 BPMs\n",
    "                while len(row) < 32:\n",
    "                    row.append('')\n",
    "\n",
    "                # Add BPM, lyrics, and sentiment\n",
    "                row.extend([\n",
    "                    sentiment_results['lyrics'],\n",
    "                    sentiment_results['sentiment']\n",
    "                ])\n",
    "\n",
    "                # Append to CSV immediately after processing each file\n",
    "                with open(output_path, 'a', newline='') as csvfile:\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "                    csv_writer.writerow(row)\n",
    "\n",
    "                # Store results (optional)\n",
    "                all_results[filename] = {\n",
    "                    'mtg_genres': mtg_genre_predictions,\n",
    "                    'essentia_genres': essentia_tagging_predictions,\n",
    "                    'moods': mood_predictions,\n",
    "                    'instruments': instrument_predictions,\n",
    "                    'key': key_n_bpm['Key'],\n",
    "                    'bpm': key_n_bpm['BPM'],\n",
    "                    'lyrics': sentiment_results['lyrics'],\n",
    "                    'sentiment': sentiment_results['sentiment']\n",
    "                }\n",
    "\n",
    "                print(f\"Processed and saved results for {filename}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534,
     "referenced_widgets": [
      "bc422f2f978140858d15b0d692d66415",
      "48623228bdc84b52bad73d3ccb209f64",
      "903dcd84226f4c85b1008a633ecf5f80",
      "7b9770b7f0cb4f2d9d2587b3cba2dd60",
      "aeae16bd9b7e4efca26c20957dbf339d",
      "177febd27b934958bc377f980ea43b12",
      "d53aaa46ffe04198b2c9127d8dcc7c51",
      "dc542cf1b38b45288eb5e7d2303c301b",
      "bbaf281f9a1f4390ac05aa3dc94e596e",
      "e4bb91eaf4b04e529f6ab41c1ea03bd3",
      "607363225d504d0d99a541ef1b14f39e",
      "735d9612a06746d1b58775ab83b5e58d",
      "d020614617b2409ea50ababa16d4d35c",
      "9bbf610c84a1477eba62a8a083db9959",
      "6eebe61fb75d4e6b8e23200e7e7fce66",
      "5ee94a1933e24e6f9ece2ed886c79cb2",
      "fde1503f7b924aafbd374b568d0f9428",
      "cc70a48595dd4a9f8c6f62ab1907d30a",
      "e341526d27b3489098005fa990816203",
      "ca1697fdde46428980c41cdc498f4c44",
      "01f8715d44604b5a9984b3448dc286cc",
      "2f52e6b6d759474ba3fd2dcc5b1444d4",
      "25f5f6a98dc54b7da21297e200d5c91c",
      "e301df6d5a234cba9e2c53edae06a9b6",
      "52af65d0a7d344f19bd7b92900010118",
      "8e365d2d94ae4588bf5b75cc3ec5976b",
      "84576bb9a1e54d1f946c3606a2af4144",
      "e747ec4798af4d4eb74d063b49a14b78",
      "ddf9d0bda3ae49cda0bbfa510effc229",
      "8cef6b5390c64fb38c3d9f38180ca884",
      "91febe4da1ff4af88aa5a07a9e65eb5a",
      "1cddfd4055fe4035af078255f24a2134",
      "15bce1da97144bcb8e8edf57e6769031",
      "4008d78bfaaf457fa0f1cec47d7c9650",
      "ce715c011a584870b26d399417b3b946",
      "267479337b684a0b862387741135cdf1",
      "a3d985e159674037bfc37cd2afe71fe8",
      "4456e428a4d5452dac5511f6f967f3bd",
      "c9bad97d75e542c684b721e2c0f2f61f",
      "905009be5caf44aa9feda9b56313945f",
      "ffd07527decf40dbbc5d8c238243c5c7",
      "a477d840188d4938b91a47986e10a0c0",
      "38eba1a0dc544a7fa0242d2649f7c31c",
      "14f54f4709e34c28a148b0f91b1b2c56"
     ]
    },
    "id": "UD__3kjuJGx_",
    "outputId": "c05b218d-d761-4d40-ec01-0bd3d83fb011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['danceability-musicnn-msd-1.pb', 'mood_aggressive-musicnn-msd-1.pb', 'mood_electronic-musicnn-msd-1.pb', 'mood_happy-musicnn-msd-1.pb', 'mood_party-musicnn-msd-1.pb', 'mood_relaxed-musicnn-msd-1.pb', 'mood_sad-musicnn-msd-1.pb']\n",
      "['danceability-musicnn-msd-1.json', 'mood_aggressive-musicnn-msd-1.json', 'mood_electronic-musicnn-msd-1.json', 'mood_happy-musicnn-msd-1.json', 'mood_party-musicnn-msd-1.json', 'mood_relaxed-musicnn-msd-1.json', 'mood_sad-musicnn-msd-1.json']\n",
      "JSON PATH: metadata/mood_detection_metadatas/danceability-musicnn-msd-1.json\n",
      "JSON PATH: metadata/mood_detection_metadatas/mood_aggressive-musicnn-msd-1.json\n",
      "JSON PATH: metadata/mood_detection_metadatas/mood_electronic-musicnn-msd-1.json\n",
      "JSON PATH: metadata/mood_detection_metadatas/mood_happy-musicnn-msd-1.json\n",
      "JSON PATH: metadata/mood_detection_metadatas/mood_party-musicnn-msd-1.json\n",
      "JSON PATH: metadata/mood_detection_metadatas/mood_relaxed-musicnn-msd-1.json\n",
      "JSON PATH: metadata/mood_detection_metadatas/mood_sad-musicnn-msd-1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 153MiB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc422f2f978140858d15b0d692d66415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735d9612a06746d1b58775ab83b5e58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f5f6a98dc54b7da21297e200d5c91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4008d78bfaaf457fa0f1cec47d7c9650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load each music analysis Model.\n",
    "\n",
    "#Create Genre Classifier.\n",
    "Genreclassifier = AudioGenreClassifier(genre_model_path=\"models/mtg_jamendo_genre-discogs-effnet-1.pb\",\n",
    "                                        model_json_path=\"metadata/mtg_jamendo_genre-discogs-effnet-1.json\",\n",
    "                                        essentia_genre_model_path=\"models/msd-musicnn-1.pb\",\n",
    "                                        essentia_genre_json_path=\"metadata/msd-musicnn-1.json\",\n",
    "                                        embedding_model_path=\"models/discogs-effnet-bs64-1.pb\")\n",
    "#Create Mood Classifier.\n",
    "MoodClassifier = AudioMoodClassifier(models_dir='models/mood_detection_models',\n",
    "                                  metadatas_dir='metadata/mood_detection_metadatas')\n",
    "\n",
    "#Create Instrument Classifier\n",
    "Instrument_classifier = AudioInstrumentClassifier(instrument_model_path=\"models/mtg_jamendo_instrument-discogs-effnet-1.pb\",\n",
    "                                        model_json_path=\"metadata/mtg_jamendo_instrument-discogs-effnet-1.json\",\n",
    "                                        embedding_model_path=\"models/discogs-effnet-bs64-1.pb\")\n",
    "#Create KeyBPMExtractor.\n",
    "KeyBPM_analyzer = KeyBPMExtractor()\n",
    "\n",
    "#Create LyricsExtractor Instance.\n",
    "Lyrics_analyzer = LyricsExtractor()\n",
    "\n",
    "# Create Sentiment analyzer\n",
    "Sentiment_analyzer = AudioSentimentAnalyzer(Lyrics_analyzer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nK61p7CdOX9s",
    "outputId": "3aab7f09-ebbf-4216-8758-6ca7d08ec1bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsm-xgybRolE",
    "outputId": "fef9a245-1cef-4b7a-b7ef-0dd9517bdf74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file with latin-1 encoding\n",
      "MTG-Genre Predictions for Eve of Enchantment.mp3: [('classical', 24.4), ('pop', 23.9), ('soundtrack', 10.5)]\n",
      "Essentia-Genre Predictions for Eve of Enchantment.mp3: [('oldies', 24.6), ('country', 16.5), ('easy listening', 12.3)]\n",
      "Mood Predictions for Eve of Enchantment.mp3: [('sad', 96.2), ('relaxed', 85.0), ('happy', 10.6)]\n",
      "Instrument Predictions for Eve of Enchantment.mp3: [('piano', 40.6), ('drums', 23.5), ('violin', 16.2)]\n",
      "Detected Key Scales are: ['D minor', 'D minor', 'D minor', 'D']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:spleeter:Downloading model archive https://github.com/deezer/spleeter/releases/download/v1.4.0/2stems.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spleeter:Downloading model archive https://github.com/deezer/spleeter/releases/download/v1.4.0/2stems.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:spleeter:Validating archive checksum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spleeter:Validating archive checksum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:spleeter:Extracting downloaded 2stems archive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spleeter:Extracting downloaded 2stems archive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:spleeter:2stems model file(s) extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spleeter:2stems model file(s) extracted\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/spleeter/separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/spleeter/separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Eve of Enchantment/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Eve of Enchantment.mp3:  The overall sentiment and mood of these lyrics are romantic and nostalgic, expressing deep affection and longing for a loved one during Christmas Eve.\n",
      "Processed and saved results for Eve of Enchantment.mp3\n",
      "MTG-Genre Predictions for Elegance in the Snow.mp3: [('classical', 22.0), ('world', 14.8), ('country', 14.4)]\n",
      "Essentia-Genre Predictions for Elegance in the Snow.mp3: [('jazz', 67.7), ('female vocalists', 27.8), ('oldies', 14.1)]\n",
      "Mood Predictions for Elegance in the Snow.mp3: [('relaxed', 94.4), ('sad', 85.8), ('electronic', 6.8)]\n",
      "Instrument Predictions for Elegance in the Snow.mp3: [('piano', 56.7), ('drums', 34.2), ('bass', 19.5)]\n",
      "Detected Key Scales are: ['Ab major', 'C minor', 'C minor', 'D#']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Elegance in the Snow/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Elegance in the Snow.mp3:  The overall sentiment and mood of these lyrics are celebratory and joyful, evoking a sense of holiday cheer and festive excitement in a bustling urban setting.\n",
      "Processed and saved results for Elegance in the Snow.mp3\n",
      "MTG-Genre Predictions for Treasures of Memory.wav: [('jazz', 32.0), ('pop', 13.4), ('blues', 10.8)]\n",
      "Essentia-Genre Predictions for Treasures of Memory.wav: [('country', 22.1), ('oldies', 21.1), ('blues', 17.7)]\n",
      "Mood Predictions for Treasures of Memory.wav: [('happy', 89.3), ('party', 30.6), ('sad', 29.0)]\n",
      "Instrument Predictions for Treasures of Memory.wav: [('drums', 37.1), ('piano', 33.1), ('bass', 25.2)]\n",
      "Detected Key Scales are: ['G major', 'G major', 'G major', 'D']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Treasures of Memory/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Treasures of Memory.wav:  The overall sentiment and mood of these lyrics evoke a sense of nostalgia and joy for the magic of childhood and Christmas memories associated with old-fashioned toy shops.\n",
      "Processed and saved results for Treasures of Memory.wav\n",
      "MTG-Genre Predictions for Pathway to Discovery.wav: [('ambient', 62.3), ('electronic', 32.9), ('chillout', 25.8)]\n",
      "Essentia-Genre Predictions for Pathway to Discovery.wav: [('ambient', 44.0), ('electronic', 18.2), ('instrumental', 15.3)]\n",
      "Mood Predictions for Pathway to Discovery.wav: [('relaxed', 99.6), ('sad', 80.2), ('electronic', 54.2)]\n",
      "Instrument Predictions for Pathway to Discovery.wav: [('synthesizer', 50.9), ('piano', 20.7), ('electricguitar', 14.2)]\n",
      "Detected Key Scales are: ['E major', 'E major', 'E major', 'B']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Pathway to Discovery/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Pathway to Discovery.wav:  The overall sentiment of these lyrics is melancholic and nostalgic, with a mood of longing for a past love.\n",
      "Processed and saved results for Pathway to Discovery.wav\n",
      "MTG-Genre Predictions for Regal Solitude.wav: [('classical', 47.2), ('pop', 21.7), ('jazz', 10.1)]\n",
      "Essentia-Genre Predictions for Regal Solitude.wav: [('jazz', 43.0), ('female vocalists', 22.2), ('soul', 8.9)]\n",
      "Mood Predictions for Regal Solitude.wav: [('relaxed', 98.4), ('sad', 97.9), ('happy', 4.9)]\n",
      "Instrument Predictions for Regal Solitude.wav: [('piano', 82.5), ('drums', 10.9), ('bass', 7.5)]\n",
      "Detected Key Scales are: ['G minor', 'G minor', 'G minor', 'G']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Regal Solitude/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Regal Solitude.wav:  The overall sentiment and mood of these lyrics is one of cold isolation and longing for warmth and love, creating a sense of melancholy and wonder.\n",
      "Processed and saved results for Regal Solitude.wav\n",
      "MTG-Genre Predictions for Eternal Vows.mp3: [('pop', 23.5), ('jazz', 21.3), ('classical', 19.7)]\n",
      "Essentia-Genre Predictions for Eternal Vows.mp3: [('oldies', 25.3), ('female vocalists', 23.8), ('jazz', 21.3)]\n",
      "Mood Predictions for Eternal Vows.mp3: [('sad', 93.1), ('relaxed', 80.4), ('happy', 22.6)]\n",
      "Instrument Predictions for Eternal Vows.mp3: [('piano', 51.5), ('drums', 21.5), ('bass', 11.5)]\n",
      "Detected Key Scales are: ['Bb major', 'Bb major', 'Bb major', 'A#']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Eternal Vows/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Eternal Vows.mp3:  The overall sentiment and mood of these lyrics are optimistic and affectionate, emphasizing the enduring power of love and commitment during the Christmas season.\n",
      "Processed and saved results for Eternal Vows.mp3\n",
      "MTG-Genre Predictions for A Night of Wonder.mp3: [('jazz', 45.3), ('swing', 11.4), ('latin', 9.3)]\n",
      "Essentia-Genre Predictions for A Night of Wonder.mp3: [('country', 31.5), ('jazz', 30.6), ('oldies', 22.2)]\n",
      "Mood Predictions for A Night of Wonder.mp3: [('happy', 84.0), ('sad', 64.4), ('relaxed', 42.0)]\n",
      "Instrument Predictions for A Night of Wonder.mp3: [('piano', 52.0), ('drums', 42.0), ('bass', 24.7)]\n",
      "Detected Key Scales are: ['C minor', 'C minor', 'C minor', 'G#']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/A Night of Wonder/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for A Night of Wonder.mp3:  The overall sentiment and mood of these lyrics is joyful and nostalgic, capturing the excitement and wonder of Christmas Eve.\n",
      "Processed and saved results for A Night of Wonder.mp3\n",
      "MTG-Genre Predictions for Embers of Remembrance.mp3: [('pop', 24.3), ('jazz', 12.3), ('classical', 12.2)]\n",
      "Essentia-Genre Predictions for Embers of Remembrance.mp3: [('oldies', 27.1), ('jazz', 18.8), ('country', 12.7)]\n",
      "Mood Predictions for Embers of Remembrance.mp3: [('sad', 98.7), ('relaxed', 95.0), ('happy', 16.9)]\n",
      "Instrument Predictions for Embers of Remembrance.mp3: [('piano', 41.1), ('drums', 22.8), ('bass', 17.7)]\n",
      "Detected Key Scales are: ['F# major', 'C# major', 'C# major', 'C#']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Embers of Remembrance/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Embers of Remembrance.mp3:  The overall sentiment and mood of these lyrics are contemplative and nostalgic, evoking a sense of warmth, comfort, and tranquility.\n",
      "Processed and saved results for Embers of Remembrance.mp3\n",
      "MTG-Genre Predictions for Beacon of Holiday Hope.mp3: [('pop', 21.2), ('folk', 16.4), ('country', 16.3)]\n",
      "Essentia-Genre Predictions for Beacon of Holiday Hope.mp3: [('country', 39.3), ('oldies', 30.0), ('jazz', 10.6)]\n",
      "Mood Predictions for Beacon of Holiday Hope.mp3: [('sad', 97.9), ('relaxed', 82.1), ('happy', 48.7)]\n",
      "Instrument Predictions for Beacon of Holiday Hope.mp3: [('piano', 48.9), ('drums', 27.1), ('bass', 17.6)]\n",
      "Detected Key Scales are: ['A major', 'A major', 'A major', 'A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Beacon of Holiday Hope/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Beacon of Holiday Hope.mp3:  The overall sentiment and mood of these lyrics are joyful and nostalgic, celebrating the beauty and magic of Christmas through the symbol of a grand and brightly lit Christmas tree.\n",
      "Processed and saved results for Beacon of Holiday Hope.mp3\n",
      "MTG-Genre Predictions for Light of Christmas.wav: [('pop', 47.8), ('jazz', 26.8), ('classical', 8.6)]\n",
      "Essentia-Genre Predictions for Light of Christmas.wav: [('soul', 34.2), ('female vocalists', 34.0), ('jazz', 22.6)]\n",
      "Mood Predictions for Light of Christmas.wav: [('sad', 82.8), ('relaxed', 78.8), ('happy', 58.2)]\n",
      "Instrument Predictions for Light of Christmas.wav: [('piano', 61.8), ('drums', 25.4), ('bass', 18.1)]\n",
      "Detected Key Scales are: ['Bb major', 'Bb major', 'Bb major', 'F']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Light of Christmas/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Light of Christmas.wav:  The overall sentiment and mood of these lyrics express a sense of hope, love, and unity during the Christmas season.\n",
      "Processed and saved results for Light of Christmas.wav\n",
      "MTG-Genre Predictions for Eternal Embrace.wav: [('pop', 21.7), ('classical', 17.7), ('folk', 9.6)]\n",
      "Essentia-Genre Predictions for Eternal Embrace.wav: [('country', 23.6), ('oldies', 13.9), ('folk', 11.3)]\n",
      "Mood Predictions for Eternal Embrace.wav: [('sad', 87.9), ('relaxed', 84.7), ('happy', 23.1)]\n",
      "Instrument Predictions for Eternal Embrace.wav: [('piano', 36.8), ('drums', 24.4), ('violin', 18.2)]\n",
      "Detected Key Scales are: ['Bb major', 'Bb major', 'Bb major', 'A#']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Eternal Embrace/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Eternal Embrace.wav:  The overall sentiment and mood of these lyrics express a longing for an everlasting love that transcends time, while grappling with the pain of hiding one's true self.\n",
      "Processed and saved results for Eternal Embrace.wav\n",
      "MTG-Genre Predictions for A Winter's Serenade.mp3: [('country', 34.7), ('folk', 29.2), ('latin', 18.1)]\n",
      "Essentia-Genre Predictions for A Winter's Serenade.mp3: [('oldies', 36.6), ('country', 36.6), ('60s', 13.7)]\n",
      "Mood Predictions for A Winter's Serenade.mp3: [('sad', 92.1), ('relaxed', 69.5), ('happy', 46.5)]\n",
      "Instrument Predictions for A Winter's Serenade.mp3: [('drums', 27.8), ('piano', 21.8), ('bass', 18.7)]\n",
      "Detected Key Scales are: ['F# minor', 'F# minor', 'F# minor', 'G#']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/A Winter's Serenade/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for A Winter's Serenade.mp3:  The overall sentiment and mood of these lyrics is one of joy, celebration, and warmth, capturing the spirit of Christmas in New York City.\n",
      "Processed and saved results for A Winter's Serenade.mp3\n",
      "MTG-Genre Predictions for Journey to the North Pole.wav: [('jazz', 76.4), ('pop', 16.5), ('jazzfusion', 11.3)]\n",
      "Essentia-Genre Predictions for Journey to the North Pole.wav: [('jazz', 52.1), ('oldies', 14.9), ('country', 13.8)]\n",
      "Mood Predictions for Journey to the North Pole.wav: [('sad', 93.9), ('relaxed', 86.1), ('happy', 19.9)]\n",
      "Instrument Predictions for Journey to the North Pole.wav: [('drums', 69.9), ('piano', 68.2), ('bass', 54.0)]\n",
      "Detected Key Scales are: ['C major', 'C major', 'C major', 'D']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Journey to the North Pole/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Journey to the North Pole.wav:  The overall sentiment and mood of these lyrics is one of excitement, wonder, and joy as they describe a magical journey to Santa's workshop on the Polar Express.\n",
      "Processed and saved results for Journey to the North Pole.wav\n",
      "MTG-Genre Predictions for Blooms of Hope.wav: [('jazz', 51.1), ('classical', 13.7), ('pop', 12.4)]\n",
      "Essentia-Genre Predictions for Blooms of Hope.wav: [('jazz', 32.6), ('female vocalists', 22.2), ('oldies', 14.8)]\n",
      "Mood Predictions for Blooms of Hope.wav: [('sad', 96.8), ('relaxed', 79.2), ('happy', 20.9)]\n",
      "Instrument Predictions for Blooms of Hope.wav: [('piano', 68.2), ('drums', 39.1), ('bass', 23.2)]\n",
      "Detected Key Scales are: ['D major', 'D major', 'D major', 'A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Blooms of Hope/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Blooms of Hope.wav:  The overall sentiment and mood of these lyrics are hopeful and peaceful, emphasizing the power of love and the beauty of the Christmas season.\n",
      "Processed and saved results for Blooms of Hope.wav\n",
      "MTG-Genre Predictions for Moments in Time.wav: [('classical', 19.0), ('pop', 13.8), ('folk', 8.1)]\n",
      "Essentia-Genre Predictions for Moments in Time.wav: [('jazz', 32.6), ('female vocalists', 21.7), ('oldies', 18.1)]\n",
      "Mood Predictions for Moments in Time.wav: [('relaxed', 97.0), ('sad', 95.1), ('happy', 17.9)]\n",
      "Instrument Predictions for Moments in Time.wav: [('piano', 31.2), ('drums', 14.2), ('bass', 9.6)]\n",
      "Detected Key Scales are: ['B major', 'B major', 'B major', 'G']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Moments in Time/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for Moments in Time.wav:  The overall sentiment and mood of these lyrics convey a sense of nostalgia, longing, and the passage of time.\n",
      "Processed and saved results for Moments in Time.wav\n",
      "MTG-Genre Predictions for Under the Christmas Star.mp3: [('classical', 27.4), ('orchestral', 11.5), ('pop', 11.2)]\n",
      "Essentia-Genre Predictions for Under the Christmas Star.mp3: [('oldies', 38.2), ('jazz', 21.3), ('easy listening', 13.4)]\n",
      "Mood Predictions for Under the Christmas Star.mp3: [('sad', 70.3), ('happy', 55.5), ('relaxed', 51.3)]\n",
      "Instrument Predictions for Under the Christmas Star.mp3: [('piano', 40.4), ('drums', 20.3), ('violin', 11.9)]\n",
      "Detected Key Scales are: ['C minor', 'C minor', 'C minor', 'G']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9545dabf2dce>:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  bpms.append(round(float(tempo)))\n",
      "<ipython-input-4-9545dabf2dce>:97: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=self.y, sr=self.sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocals Audio File Path:/content/audio_analysis_temp/separated/Under the Christmas Star/vocals.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "#Store results of without vocals in a CSV.\n",
    "process_directory(audio_dir='/content/drive/MyDrive/Vocals/Vocals', # Give Path to Audio Files (Vocals or Without Vocals.)\n",
    "                  output_path='/content/drive/MyDrive/vocals_output_v2.csv', #Output CSV Path.\n",
    "                  genre_classifier=Genreclassifier,\n",
    "                  mood_classifier=MoodClassifier,\n",
    "                  Instrument_classifier=Instrument_classifier,\n",
    "                  KeyBPM=KeyBPM_analyzer,\n",
    "                  Sentiment_analyzer=Sentiment_analyzer)\n",
    "\n",
    "#Store Results of With vocals in a CSV.\n",
    "\n",
    "# process_directory(audio_dir='audio_files',\n",
    "#                   output_dir='without_vocals',\n",
    "#                   genre_classifier=Genreclassifier,\n",
    "#                   mood_classifier=MoodClassifier,\n",
    "#                   Instrument_classifier=Instrument_classifier,\n",
    "#                   KeyBPM=KeyBPM_analyzer,\n",
    "#                   Sentiment_analyzer=Sentiment_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFLQ1nOlH1Y4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2e5xeprR2p7B",
    "KFgTSsex8jo_"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "audio_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01f8715d44604b5a9984b3448dc286cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14f54f4709e34c28a148b0f91b1b2c56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "15bce1da97144bcb8e8edf57e6769031": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "177febd27b934958bc377f980ea43b12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cddfd4055fe4035af078255f24a2134": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25f5f6a98dc54b7da21297e200d5c91c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e301df6d5a234cba9e2c53edae06a9b6",
       "IPY_MODEL_52af65d0a7d344f19bd7b92900010118",
       "IPY_MODEL_8e365d2d94ae4588bf5b75cc3ec5976b"
      ],
      "layout": "IPY_MODEL_84576bb9a1e54d1f946c3606a2af4144"
     }
    },
    "267479337b684a0b862387741135cdf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffd07527decf40dbbc5d8c238243c5c7",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a477d840188d4938b91a47986e10a0c0",
      "value": 231508
     }
    },
    "2f52e6b6d759474ba3fd2dcc5b1444d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "38eba1a0dc544a7fa0242d2649f7c31c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4008d78bfaaf457fa0f1cec47d7c9650": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce715c011a584870b26d399417b3b946",
       "IPY_MODEL_267479337b684a0b862387741135cdf1",
       "IPY_MODEL_a3d985e159674037bfc37cd2afe71fe8"
      ],
      "layout": "IPY_MODEL_4456e428a4d5452dac5511f6f967f3bd"
     }
    },
    "4456e428a4d5452dac5511f6f967f3bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48623228bdc84b52bad73d3ccb209f64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_177febd27b934958bc377f980ea43b12",
      "placeholder": "​",
      "style": "IPY_MODEL_d53aaa46ffe04198b2c9127d8dcc7c51",
      "value": "config.json: 100%"
     }
    },
    "52af65d0a7d344f19bd7b92900010118": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cef6b5390c64fb38c3d9f38180ca884",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91febe4da1ff4af88aa5a07a9e65eb5a",
      "value": 48
     }
    },
    "5ee94a1933e24e6f9ece2ed886c79cb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "607363225d504d0d99a541ef1b14f39e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6eebe61fb75d4e6b8e23200e7e7fce66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01f8715d44604b5a9984b3448dc286cc",
      "placeholder": "​",
      "style": "IPY_MODEL_2f52e6b6d759474ba3fd2dcc5b1444d4",
      "value": " 268M/268M [00:01&lt;00:00, 157MB/s]"
     }
    },
    "735d9612a06746d1b58775ab83b5e58d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d020614617b2409ea50ababa16d4d35c",
       "IPY_MODEL_9bbf610c84a1477eba62a8a083db9959",
       "IPY_MODEL_6eebe61fb75d4e6b8e23200e7e7fce66"
      ],
      "layout": "IPY_MODEL_5ee94a1933e24e6f9ece2ed886c79cb2"
     }
    },
    "7b9770b7f0cb4f2d9d2587b3cba2dd60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4bb91eaf4b04e529f6ab41c1ea03bd3",
      "placeholder": "​",
      "style": "IPY_MODEL_607363225d504d0d99a541ef1b14f39e",
      "value": " 629/629 [00:00&lt;00:00, 24.8kB/s]"
     }
    },
    "84576bb9a1e54d1f946c3606a2af4144": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cef6b5390c64fb38c3d9f38180ca884": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e365d2d94ae4588bf5b75cc3ec5976b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cddfd4055fe4035af078255f24a2134",
      "placeholder": "​",
      "style": "IPY_MODEL_15bce1da97144bcb8e8edf57e6769031",
      "value": " 48.0/48.0 [00:00&lt;00:00, 2.58kB/s]"
     }
    },
    "903dcd84226f4c85b1008a633ecf5f80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc542cf1b38b45288eb5e7d2303c301b",
      "max": 629,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bbaf281f9a1f4390ac05aa3dc94e596e",
      "value": 629
     }
    },
    "905009be5caf44aa9feda9b56313945f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91febe4da1ff4af88aa5a07a9e65eb5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9bbf610c84a1477eba62a8a083db9959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e341526d27b3489098005fa990816203",
      "max": 267832558,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca1697fdde46428980c41cdc498f4c44",
      "value": 267832558
     }
    },
    "a3d985e159674037bfc37cd2afe71fe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38eba1a0dc544a7fa0242d2649f7c31c",
      "placeholder": "​",
      "style": "IPY_MODEL_14f54f4709e34c28a148b0f91b1b2c56",
      "value": " 232k/232k [00:00&lt;00:00, 673kB/s]"
     }
    },
    "a477d840188d4938b91a47986e10a0c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aeae16bd9b7e4efca26c20957dbf339d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbaf281f9a1f4390ac05aa3dc94e596e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bc422f2f978140858d15b0d692d66415": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48623228bdc84b52bad73d3ccb209f64",
       "IPY_MODEL_903dcd84226f4c85b1008a633ecf5f80",
       "IPY_MODEL_7b9770b7f0cb4f2d9d2587b3cba2dd60"
      ],
      "layout": "IPY_MODEL_aeae16bd9b7e4efca26c20957dbf339d"
     }
    },
    "c9bad97d75e542c684b721e2c0f2f61f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca1697fdde46428980c41cdc498f4c44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc70a48595dd4a9f8c6f62ab1907d30a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce715c011a584870b26d399417b3b946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9bad97d75e542c684b721e2c0f2f61f",
      "placeholder": "​",
      "style": "IPY_MODEL_905009be5caf44aa9feda9b56313945f",
      "value": "vocab.txt: 100%"
     }
    },
    "d020614617b2409ea50ababa16d4d35c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fde1503f7b924aafbd374b568d0f9428",
      "placeholder": "​",
      "style": "IPY_MODEL_cc70a48595dd4a9f8c6f62ab1907d30a",
      "value": "model.safetensors: 100%"
     }
    },
    "d53aaa46ffe04198b2c9127d8dcc7c51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc542cf1b38b45288eb5e7d2303c301b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddf9d0bda3ae49cda0bbfa510effc229": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e301df6d5a234cba9e2c53edae06a9b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e747ec4798af4d4eb74d063b49a14b78",
      "placeholder": "​",
      "style": "IPY_MODEL_ddf9d0bda3ae49cda0bbfa510effc229",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "e341526d27b3489098005fa990816203": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4bb91eaf4b04e529f6ab41c1ea03bd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e747ec4798af4d4eb74d063b49a14b78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fde1503f7b924aafbd374b568d0f9428": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffd07527decf40dbbc5d8c238243c5c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
